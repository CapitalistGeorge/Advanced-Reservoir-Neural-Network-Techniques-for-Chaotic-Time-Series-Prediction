{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymssa import MSSA\n",
    "from src.asc_itmo_lab.chaos_indic import HurstTraj, NoiseFactor, DimEmb, max_lyapunov, fourier_harmonic_count, ks_entropy_partition\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets = pd.read_csv('/Users/roman14/Downloads/Reservoir-Computing-Investigation/M4 Competition Dataset/m4_info.csv')\n",
    "m4_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets['category'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets['SP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets.groupby(by=['category','SP']).count()['M4id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_train_csv_files(directory='.'):\n",
    "    \"\"\"\n",
    "    Lists all files in the given directory that end with '-train.csv' and returns their count and names.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory to search. Defaults to current directory.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (count of matching files, list of matching filenames)\n",
    "    \"\"\"\n",
    "    files = os.listdir(directory)\n",
    "    train_files = [f for f in files if f.endswith('-train.csv')]\n",
    "    return len(train_files), train_files\n",
    "\n",
    "\n",
    "len_train_files, files = list_train_csv_files(os.getcwd() + '/M4 Competition Dataset')\n",
    "print(f\"Found {len_train_files} '-train.csv' files:\")\n",
    "for file in files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find series with length at least 1000 points\n",
    "ts_id = []\n",
    "time_series_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(os.getcwd() + '/M4 Competition Dataset', file)).T\n",
    "\n",
    "    if df.shape[0] > 1000:\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df.columns = df.iloc[0]          # first row becomes header\n",
    "        df.drop(df.index[0], inplace=True)\n",
    "        notna=df.notna().sum()\n",
    "        ts = notna.keys()[np.where(notna.values>1000)]\n",
    "        ts_id.append(ts.to_list())\n",
    "        filtered_series = df[ts].dropna()\n",
    "        time_series_df = pd.concat([time_series_df, filtered_series], axis=1)\n",
    "\n",
    "time_series_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = time_series_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ids = np.concatenate(ts_id).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets_cleaned = m4_datasets[m4_datasets['M4id'].isin(ts_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired proportions and counts for 650 samples\n",
    "proportions = m4_datasets_cleaned['category'].value_counts(normalize=True).to_dict()\n",
    "total_to_sample = 1501\n",
    "desired_counts = {cat: int(round(p * total_to_sample)) for cat, p in proportions.items()}\n",
    "\n",
    "# Sample by category\n",
    "selected_frames = []\n",
    "for cat, count in desired_counts.items():\n",
    "    pool = m4_datasets_cleaned[m4_datasets_cleaned['category'] == cat]\n",
    "    selected = pool.sample(n=count, random_state=23)\n",
    "    selected_frames.append(selected)\n",
    "\n",
    "selected_info = pd.concat(selected_frames).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets_cleaned['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_datasets_cleaned['SP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_series(file_list, min_obs=850, target_n=650):\n",
    "#     \"\"\"\n",
    "#     Iterate over CSVs in file_list, selecting columns with at least min_obs\n",
    "#     non-NaN values, until we have target_n series. Returns a DataFrame whose\n",
    "#     columns are the selected series.\n",
    "    \n",
    "#     file_list : list of filenames (strings)\n",
    "#     min_obs    : minimum non-NaN count per series\n",
    "#     target_n   : total series to collect\n",
    "#     \"\"\"\n",
    "#     collected = pd.DataFrame()  # will hold selected columns\n",
    "#     collected_count = 0\n",
    "    \n",
    "#     for fname in file_list:\n",
    "#         if collected_count >= target_n:\n",
    "#             break\n",
    "        \n",
    "#         # 1) Load and promote row 0 to header\n",
    "#         df = pd.read_csv(os.path.join(os.getcwd() + '/M4 Competition Dataset', fname))\n",
    "#         df = df.T\n",
    "#         df.reset_index(inplace=True, drop=True)\n",
    "#         df.columns = df.iloc[0]          # first row becomes header\n",
    "#         df.drop(df.index[0], inplace=True)\n",
    "        \n",
    "#         # 2) For each column, count non-NaNs\n",
    "#         for col in df.columns:\n",
    "#             if collected_count >= target_n:\n",
    "#                 break\n",
    "            \n",
    "#             non_nans = df[col].notna().sum()\n",
    "#             if non_nans >= min_obs:\n",
    "#                 # add this column to collected (align on index length)\n",
    "#                 # pad shorter series with NaN if needed\n",
    "#                 series = df[col]\n",
    "#                 if len(series) < min_obs:\n",
    "#                     continue  # shouldn't happen, but guard\n",
    "#                 # if collected is empty, initialize with index\n",
    "#                 if collected_count == 0:\n",
    "#                     collected = pd.DataFrame(index=range(len(series)))\n",
    "#                 # pad/truncate to collected.index length\n",
    "#                 L = len(collected)\n",
    "#                 s = series.reset_index(drop=True)\n",
    "#                 if len(s) > L:\n",
    "#                     s = s.iloc[:L]\n",
    "#                 elif len(s) < L:\n",
    "#                     s = s.reindex(range(L))\n",
    "#                 collected[col] = s\n",
    "#                 collected_count += 1\n",
    "        \n",
    "#     print(f\"Collected {collected_count} series out of requested {target_n}.\")\n",
    "#     return collected\n",
    "\n",
    "# result_df = collect_series(files, min_obs=1000, target_n=1500)\n",
    "# print(\"Resulting DataFrame shape:\", result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_df = result_df.dropna()\n",
    "# time_series_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in time_series_df.columns:\n",
    "    try:\n",
    "        time_series_df[col] = pd.to_numeric(time_series_df[col], errors='raise')\n",
    "    except ValueError:\n",
    "        # skip columns that aren’t pure numbers\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = time_series_df[selected_info['M4id'].to_list()]\n",
    "time_series_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_df.to_csv('time_series.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(time_series_df['W19'])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Timestep')\n",
    "plt.title('W19 time-series')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(time_series_df['W19'])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Timestep')\n",
    "plt.title('W19 time-series')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(time_series_df['D2452'])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Timestep')\n",
    "plt.title('D2452 time-series')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimations = []\n",
    "\n",
    "for series_name in tqdm(time_series_df.columns):\n",
    "    # Only process if enough data points\n",
    "    if len(time_series_df[series_name]) < 1:\n",
    "        continue\n",
    "    \n",
    "    # Compute metrics\n",
    "    _, _, hurst_exponent, _ = HurstTraj(time_series_df[series_name])\n",
    "    noise = NoiseFactor(time_series_df[series_name])\n",
    "    _, corr_dim, _ = DimEmb(time_series_df[series_name])\n",
    "    lyap_max = max_lyapunov(time_series_df[series_name].values)\n",
    "    ks_ent = ks_entropy_partition(time_series_df[series_name])\n",
    "    fourier_count = fourier_harmonic_count(time_series_df[series_name])\n",
    "    \n",
    "    estimations.append({\n",
    "        'series': series_name,\n",
    "        'hurst': hurst_exponent,\n",
    "        'noise_factor': noise,\n",
    "        'corr_dim': corr_dim,\n",
    "        'lyap_max': lyap_max,\n",
    "        'ks_entropy': ks_ent,\n",
    "        'fourier_harmonics': fourier_count\n",
    "    })\n",
    "\n",
    "measures_df = pd.DataFrame(estimations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.to_csv('measures_df.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    row = i // 3\n",
    "    col_num = i % 3\n",
    "    ax = axes[row, col_num]\n",
    "    \n",
    "    sns.kdeplot(data=measures_df[col], ax=ax, fill=True, color='skyblue', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    median_val = measures_df[col].median()\n",
    "    mean_val = measures_df[col].mean()\n",
    "    \n",
    "    ax.axvline(median_val, color='red', linestyle='--', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
    "    ax.axvline(mean_val, color='green', linestyle='-', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(f'Distribution of {col}', fontsize=12, pad=10)\n",
    "    ax.set_xlabel('Value', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "if len(columns) < 6:\n",
    "    for i in range(len(columns), 6):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']\n",
    "\n",
    "# Создаем фигуру с несколькими subplots\n",
    "fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(10, 2 * len(columns)))\n",
    "\n",
    "# Проходим по всем колонкам и строим KDE для каждой\n",
    "for i, col in enumerate(columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Строим KDE\n",
    "    sns.kdeplot(data=measures_df[col], ax=ax, fill=True, alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    # Вычисляем медиану и среднее\n",
    "    median_val = measures_df[col].median()\n",
    "    mean_val = measures_df[col].mean()\n",
    "    \n",
    "    # Добавляем вертикальные линии для медианы и среднего\n",
    "    ax.axvline(median_val, color='red', linestyle='--', linewidth=1, label=f'Median: {median_val:.2f}')\n",
    "    ax.axvline(mean_val, color='green', linestyle='-', linewidth=1, label=f'Mean: {mean_val:.2f}')\n",
    "    \n",
    "    # Добавляем легенду и заголовок\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(x, test_step):\n",
    "    X_train = x[:-test_step-1]\n",
    "    Y_train = x[1:-test_step]\n",
    "    y_test = x[-test_step:]\n",
    "\n",
    "    return X_train, Y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# ignore just the specific DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "# ignore the “X does not have valid feature names” UserWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names\"\n",
    ")\n",
    "\n",
    "l_res_lgbm = []\n",
    "time_spent_list = []\n",
    "\n",
    "for ts in tqdm(measures_df['series']):\n",
    "    start_time = time.time()\n",
    "\n",
    "    lgbm = LGBMRegressor(random_state=23, verbosity=-1)\n",
    "    l_res_ts = []\n",
    "    for i in range(15, -1, -1):\n",
    "        X_train, Y_train, y_test = train_test(time_series_df[ts], test_step=15+i)\n",
    "        lgbm = lgbm.fit(X_train.values.reshape(-1,1), Y_train.values.ravel())\n",
    "        \n",
    "        cur = Y_train.values[-1]\n",
    "        preds = []\n",
    "        for j in range(15):\n",
    "            cur = lgbm.predict(cur.reshape(1, -1)).reshape(-1, 1)[0]\n",
    "            preds.append(cur)\n",
    "        \n",
    "        l_res_ts.append(mean_absolute_percentage_error(y_test[:15], preds)*100)\n",
    "    \n",
    "    l_res_lgbm.append(np.mean(l_res_ts))\n",
    "    end_time = time.time()  # Засекаем время окончания обработки серии\n",
    "    time_spent = end_time - start_time  # Вычисляем общее время выполнения\n",
    "    time_spent_list.append(time_spent)  # Добавляем в список\n",
    "\n",
    "measures_df['mape_lightgbm'] = l_res_lgbm\n",
    "measures_df['time_spent_lightgbm'] = time_spent_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=l_res_lgbm, orient=\"x\")\n",
    "plt.title('Box-Plot of LightGBM MAPE predictions')\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('MAPE, %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(l_res_lgbm),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['hurst'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('Hurst exponent')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,20))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['ks_entropy'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('KS entropy')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,20))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['noise_factor'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('Noise measure (big-better)')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,50))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['fourier_harmonics'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('Num of Fourier Harmonics')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,50))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['corr_dim'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('Correlation dimension')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,50))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['lyap_max'], measures_df['mape_lightgbm'])\n",
    "plt.xlabel('Lyapunov Exponent')\n",
    "plt.ylabel('LightGBM MAPE')\n",
    "plt.ylim((0,50))\n",
    "plt.xlim((0, 0.2))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## MSSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_res_mssa = []\n",
    "time_spent_list = []\n",
    "for ts in tqdm(measures_df['series']):\n",
    "    start_time = time.time()\n",
    "    mssa = MSSA(verbose=False)\n",
    "    l_res_ts = []\n",
    "    for i in range(15, -1, -1):\n",
    "        _, _, y_test = train_test(time_series_df[ts], test_step=15+i)\n",
    "        mssa.fit(time_series_df[ts][:-15-i])\n",
    "\n",
    "        preds = mssa.forecast(15).ravel()\n",
    "\n",
    "        l_res_ts.append(mean_absolute_percentage_error(y_test[:15], preds)*100)\n",
    "    \n",
    "    l_res_mssa.append(np.mean(l_res_ts))\n",
    "    end_time = time.time()  # Засекаем время окончания обработки серии\n",
    "    time_spent = end_time - start_time  # Вычисляем общее время выполнения\n",
    "    time_spent_list.append(time_spent)  # Добавляем в список\n",
    "\n",
    "measures_df['mape_mssa'] = l_res_mssa\n",
    "measures_df['time_spent_mssa'] = time_spent_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=measures_df['mape_mssa'], orient=\"x\")\n",
    "plt.title('Box-Plot of MSSA MAPE')\n",
    "plt.xlim((0,35))\n",
    "plt.ylabel('MAPE, %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(measures_df['mape_mssa']),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import warnings\n",
    "\n",
    "# Silence warnings from Prophet\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_prophet_df(series, start, freq):\n",
    "    \"\"\"Convert your numeric series to a DataFrame compatible with Prophet.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'ds': pd.date_range(start=start, periods=len(series), freq=freq),\n",
    "        'y': series\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def determine_freq(ts, starting_date):\n",
    "    if ts.startswith('H'):\n",
    "        freq ='H'\n",
    "    elif ts.startswith('D'):\n",
    "        freq = 'D'\n",
    "    elif ts.startswith('W'):\n",
    "        wd = pd.to_datetime(starting_date, dayfirst=True).weekday()\n",
    "        freq = ['W-MON','W-TUE','W-WED','W-THU','W-FRI','W-SAT','W-SUN'][wd]\n",
    "    elif ts.startswith('M'):\n",
    "        if pd.to_datetime(starting_date, dayfirst=True).day > 1:\n",
    "            freq = 'ME'\n",
    "        else:\n",
    "            freq = 'MS'\n",
    "    elif ts.startswith('Q'):\n",
    "        if pd.to_datetime(starting_date, dayfirst=True).day > 1:\n",
    "            freq = 'QE'\n",
    "        else:\n",
    "            freq = 'QS'\n",
    "    elif ts.startswith('Y'):\n",
    "        if pd.to_datetime(starting_date, dayfirst=True).day > 1:\n",
    "            freq = 'YE'\n",
    "        else:\n",
    "            freq = 'YS'\n",
    "\n",
    "    return freq\n",
    "\n",
    "l_res_prophet = []\n",
    "time_spent_list = []\n",
    "for ts in tqdm(measures_df['series']):\n",
    "    start_time = time.time()\n",
    "    l_res_ts = []\n",
    "\n",
    "    # Fetch your time series and remove NaNs\n",
    "    series_data = time_series_df[ts].dropna().values\n",
    "\n",
    "    for i in range(15, -1, -1):\n",
    "        test_step = 15 + i\n",
    "        train_series = series_data[:-test_step]\n",
    "        y_test = series_data[-test_step:][:15]\n",
    "\n",
    "        # Prepare data for Prophet\n",
    "        starting_date = m4_datasets['StartingDate'].loc[m4_datasets['M4id'] == ts].item()\n",
    "        freq = determine_freq(ts, starting_date)\n",
    "        train_df = prepare_prophet_df(train_series, start=starting_date, freq=freq)\n",
    "\n",
    "        # Initialize and fit the Prophet model\n",
    "        model = Prophet()\n",
    "        model.fit(train_df)\n",
    "\n",
    "        # Predict 10 future steps\n",
    "        future = model.make_future_dataframe(periods=15, freq=freq)  # or your appropriate frequency\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        preds = forecast['yhat'].values[-15:]  # Take last 15 predictions\n",
    "\n",
    "        # Evaluate predictions\n",
    "        mape = mean_absolute_percentage_error(y_test, preds) * 100\n",
    "        l_res_ts.append(mape)\n",
    "\n",
    "    l_res_prophet.append(np.mean(l_res_ts))\n",
    "    end_time = time.time()  # Засекаем время окончания обработки серии\n",
    "    time_spent = end_time - start_time  # Вычисляем общее время выполнения\n",
    "    time_spent_list.append(time_spent)  # Добавляем в список\n",
    "\n",
    "\n",
    "measures_df['mape_prophet'] = l_res_prophet\n",
    "measures_df['time_spent_prophet'] = time_spent_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=measures_df['mape_prophet'], orient=\"x\")\n",
    "plt.title('Box-Plot of Prophet MAPE')\n",
    "plt.xlim((0,25))\n",
    "plt.ylabel('MAPE, %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(measures_df['mape_prophet']),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reservoirpy as rpy\n",
    "from reservoirpy.nodes import Ridge, Reservoir\n",
    "rpy.verbosity(0)  # no need to be too verbose here\n",
    "rpy.set_seed(23)  # make everything reproducible!\n",
    "\n",
    "units = 100\n",
    "leak_rate = 0.3\n",
    "spectral_radius = 0.9\n",
    "input_scaling = 1.0\n",
    "connectivity = 0.1\n",
    "input_connectivity = 0.2\n",
    "regularization = 1e-8\n",
    "seed = 23\n",
    "\n",
    "def reset_esn():\n",
    "    reservoir = Reservoir(units, input_scaling=input_scaling, sr=spectral_radius,\n",
    "                        lr=leak_rate, rc_connectivity=connectivity,\n",
    "                        input_connectivity=input_connectivity, seed=seed)\n",
    "\n",
    "    readout = Ridge(ridge=regularization)\n",
    "\n",
    "    return reservoir >> readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_res_esn = []\n",
    "# for ts in tqdm(measures_df['series']):\n",
    "#     l_res_ts = []\n",
    "#     for i in range(10, -1, -1):\n",
    "#         X_train, Y_train, y_test = train_test(time_series_df[ts], test_step=10+i)\n",
    "#         esn = reset_esn()\n",
    "#         esn = esn.fit(X_train.values.reshape(-1, 1), Y_train.values.reshape(-1, 1))\n",
    "        \n",
    "#         cur = Y_train.values.reshape(-1, 1)[-1]\n",
    "#         preds = []\n",
    "#         for j in range(10):\n",
    "#             cur = esn.run(cur).ravel()[0]\n",
    "#             preds.append(cur)\n",
    "        \n",
    "#         l_res_ts.append(mean_absolute_percentage_error(y_test[:10], preds)*100)\n",
    "    \n",
    "#     l_res_esn.append(np.mean(l_res_ts))\n",
    "# measures_df['mape_esn'] = l_res_esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute log-returns\n",
    "def to_log_return(x):\n",
    "    return np.diff(np.log(x))\n",
    "\n",
    "# Function to reconstruct original prices from log-returns\n",
    "def from_log_return(last_known_value, log_returns):\n",
    "    return last_known_value * np.exp(np.cumsum(log_returns))\n",
    "\n",
    "l_res_esn = []\n",
    "time_spent_list = []\n",
    "for ts in tqdm(measures_df['series']):\n",
    "    start_time = time.time()\n",
    "    l_res_ts = []\n",
    "    series_data = time_series_df[ts].dropna().values\n",
    "\n",
    "    for i in range(15, -1, -1):\n",
    "        test_step = 15 + i\n",
    "        \n",
    "        # split data into train-test\n",
    "        train_raw = series_data[:-test_step]\n",
    "        y_test_real = series_data[-test_step:][:15]\n",
    "\n",
    "        # convert train data to log-returns\n",
    "        train_log_returns = to_log_return(train_raw)\n",
    "\n",
    "        # ESN training (assuming reset_esn gives a correctly initialized ESN model)\n",
    "        esn = reset_esn()\n",
    "        \n",
    "        # prepare inputs for ESN (X_train and Y_train as sequential data)\n",
    "        X_train = train_log_returns[:-1].reshape(-1, 1)\n",
    "        Y_train = train_log_returns[1:].reshape(-1, 1)\n",
    "        \n",
    "        esn.fit(X_train, Y_train)\n",
    "\n",
    "        # Forecasting\n",
    "        preds_log_return = []\n",
    "        current_input = Y_train[-1].reshape(1, -1)\n",
    "\n",
    "        for j in range(15):\n",
    "            pred_log_ret = esn.run(current_input).item()\n",
    "            preds_log_return.append(pred_log_ret)\n",
    "            current_input = np.array([[pred_log_ret]])\n",
    "\n",
    "        # reconstruct predictions from log-returns to real data\n",
    "        preds_real = from_log_return(train_raw[-1], preds_log_return)\n",
    "\n",
    "        # calculate MAPE on real data\n",
    "        mape = mean_absolute_percentage_error(y_test_real, preds_real) * 100\n",
    "        l_res_ts.append(mape)\n",
    "\n",
    "    l_res_esn.append(np.mean(l_res_ts))\n",
    "    end_time = time.time()  # Засекаем время окончания обработки серии\n",
    "    time_spent = end_time - start_time  # Вычисляем общее время выполнения\n",
    "    time_spent_list.append(time_spent)  # Добавляем в список\n",
    "\n",
    "    \n",
    "measures_df['mape_esn'] = l_res_esn\n",
    "measures_df['time_spent_esn'] = time_spent_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=measures_df['mape_esn'], orient=\"x\")\n",
    "plt.title('Box-Plot of ESN MAPE predictions')\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('MAPE, %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(l_res_esn),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['hurst'], l_res_esn)\n",
    "plt.xlabel('Hurst exponent')\n",
    "plt.ylabel('ESN MAPE')\n",
    "plt.ylim((0,20))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(measures_df['ks_entropy'], l_res_esn)\n",
    "plt.xlabel('KS entropy')\n",
    "plt.ylabel('ESN MAPE')\n",
    "plt.ylim((0,20))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Fourie ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge as Ridge_sklearn\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "class EnhancedESN_FAN:\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 reservoir_size=100,\n",
    "                 spectral_radius=0.9,\n",
    "                 sparsity=0.1,\n",
    "                 ridge_alpha=1.0,\n",
    "                 poly_order=2,\n",
    "                 fan_dp=5,\n",
    "                 seed=23):\n",
    "        \"\"\"\n",
    "        input_dim : dimensionality of X (features per timestep)\n",
    "        fan_dp    : number of Fourier terms (dp in the FAN paper)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.poly = PolynomialFeatures(degree=poly_order, include_bias=False)\n",
    "        self.fan_dp = fan_dp\n",
    "\n",
    "        # placeholders; actual Win/W init happens in fit()\n",
    "        self.Win = None\n",
    "        self.W   = None\n",
    "\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        self.ridge = Ridge_sklearn(alpha=ridge_alpha)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.input_scaler = StandardScaler()\n",
    "\n",
    "    def initialize_weights(self, X):\n",
    "        # input‐distribution aware Win\n",
    "        inp_mean, inp_std = X.mean(axis=0), X.std(axis=0)\n",
    "        mu, sigma = inp_mean.mean(), inp_std.mean()\n",
    "        self.Win = np.random.normal(mu, sigma, (self.reservoir_size, X.shape[1] + 1))\n",
    "\n",
    "        # sparse W with spectral radius\n",
    "        W = np.random.rand(self.reservoir_size, self.reservoir_size) - 0.5\n",
    "        W[np.random.rand(*W.shape) > self.sparsity] = 0\n",
    "        W *= self.spectral_radius / np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        self.W = W\n",
    "\n",
    "    def _update(self, state, u):\n",
    "        # standard ESN update\n",
    "        pre = self.Win @ np.concatenate(([1], u)) + self.W @ state\n",
    "        return np.tanh(pre)\n",
    "\n",
    "    def _fourier_features(self, X):\n",
    "        # X: (n_samples, n_features)\n",
    "        # return: (n_samples, 2*fan_dp*n_features)\n",
    "        # FAN: explicit Fourier series terms sin(2πk x), cos(2πk x)\n",
    "        features = []\n",
    "        for k in range(1, self.fan_dp + 1):\n",
    "            features.append(np.sin(2 * np.pi * k * X))\n",
    "            features.append(np.cos(2 * np.pi * k * X))\n",
    "        return np.hstack(features)\n",
    "    \n",
    "    # def _fft_features(self, X, n_coeffs):\n",
    "    #     # X: (T, n) time series\n",
    "    #     # return first n_coeffs magnitudes per dimension\n",
    "    #     coeffs = np.abs(fft(X, axis=0))[:n_coeffs]  # shape (n_coeffs, n)\n",
    "    #     return np.tile(coeffs.flatten(), (X.shape[0], 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: shape (T, input_dim)\n",
    "        y: shape (T,) or (T, output_dim)\n",
    "        \"\"\"\n",
    "        # init reservoir\n",
    "        self.initialize_weights(X)\n",
    "\n",
    "        # collect reservoir states\n",
    "        T = X.shape[0]\n",
    "        states = np.zeros((T, self.reservoir_size))\n",
    "        state = np.zeros(self.reservoir_size)\n",
    "        for t in range(T):\n",
    "            state = self._update(state, X[t])\n",
    "            states[t] = state\n",
    "\n",
    "        X_scaled = self.input_scaler.fit_transform(X)\n",
    "        # polynomial features\n",
    "        P = self.poly.fit_transform(X_scaled)\n",
    "        # Fourier analysis network features\n",
    "        F = self._fourier_features(X_scaled)\n",
    "\n",
    "        # # combine all\n",
    "        # H = np.hstack([states, P, F])\n",
    "\n",
    "        # # scale & fit readout\n",
    "        # Hs = self.scaler.fit_transform(H)\n",
    "        # self.ridge.fit(Hs, y)\n",
    "\n",
    "        H = np.hstack([states, P, F])\n",
    "        self.scaler.fit(H)\n",
    "        # prevent division by zero in .transform()\n",
    "        self.scaler.scale_[self.scaler.scale_ == 0.0] = 1.0\n",
    "        Hs = self.scaler.transform(H)\n",
    "        self.ridge.fit(Hs, y)\n",
    "\n",
    "\n",
    "    def predict(self, X, generative_steps=None):\n",
    "        \"\"\"\n",
    "        If X is (T, input_dim) and generative_steps is None, this is open-loop:\n",
    "          returns readout(X)\n",
    "        If generative_steps is an int > 0, we perform:\n",
    "          - use X[-1] as u0, then recursively predict next u's.\n",
    "        \"\"\"\n",
    "        if generative_steps is None:\n",
    "            # one‐shot prediction (teacher forcing)\n",
    "            T = X.shape[0]\n",
    "            states = np.zeros((T, self.reservoir_size))\n",
    "            state = np.zeros(self.reservoir_size)\n",
    "            for t in range(T):\n",
    "                state = self._update(state, X[t])\n",
    "                states[t] = state\n",
    "\n",
    "            P = self.poly.transform(X)\n",
    "            F = self._fourier_features(X)\n",
    "            H = np.hstack([states, P, F])\n",
    "            Hs = self.scaler.transform(H)\n",
    "            return self.ridge.predict(Hs)\n",
    "\n",
    "        # generative forecasting\n",
    "        u = X[-1].copy()      # last observed input\n",
    "        state = np.zeros(self.reservoir_size)\n",
    "        preds = []\n",
    "        for _ in range(generative_steps):\n",
    "            state = self._update(state, u.reshape(-1))\n",
    "            # build feature row\n",
    "            # P = self.poly.transform(u.reshape(1, -1))\n",
    "            # F = self._fourier_features(u.reshape(1, -1))\n",
    "            u_scaled = self.input_scaler.transform(u.reshape(1, -1))\n",
    "            clip = 3.0  # you can tune this (e.g. 3 standard deviations)\n",
    "            u_scaled = np.clip(u_scaled, -clip, clip)\n",
    "\n",
    "            P = self.poly.transform(u_scaled)\n",
    "            F = self._fourier_features(u_scaled)\n",
    "            h = np.hstack([state, P.ravel(), F.ravel()]).reshape(1, -1)\n",
    "            h_s = self.scaler.transform(h)\n",
    "            u_next = self.ridge.predict(h_s)\n",
    "            preds.append(u_next.ravel())\n",
    "            u = u_next  # feed back\n",
    "\n",
    "        return np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_res_fourie_esn = []\n",
    "time_spent_list = []\n",
    "for ts in tqdm(measures_df['series']):\n",
    "    start_time = time.time()\n",
    "    l_res_ts = []\n",
    "    for i in range(15, -1, -1):\n",
    "        X_train, Y_train, y_test = train_test(time_series_df[ts], test_step=15+i)\n",
    "        esn_fan = EnhancedESN_FAN(input_dim=1)\n",
    "        esn_fan.fit(X=X_train.values.reshape(-1,1), y=Y_train.values.reshape(-1,1))\n",
    "        \n",
    "        preds = esn_fan.predict(Y_train.values.reshape(-1,1), generative_steps=15)\n",
    "        \n",
    "        l_res_ts.append(mean_absolute_percentage_error(y_test[:15], preds)*100)\n",
    "    \n",
    "    l_res_fourie_esn.append(np.mean(l_res_ts))\n",
    "    end_time = time.time()  # Засекаем время окончания обработки серии\n",
    "    time_spent = end_time - start_time  # Вычисляем общее время выполнения\n",
    "    time_spent_list.append(time_spent)  # Добавляем в список\n",
    "\n",
    "measures_df['mape_esn_fan'] = l_res_fourie_esn\n",
    "measures_df['time_spent_esn_f'] = time_spent_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=measures_df['mape_esn_fan'], orient=\"x\")\n",
    "plt.title('Box-Plot of ESN_FAN MAPE')\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('MAPE, %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(measures_df['mape_esn_fan']),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.to_csv('measures_df.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "# Box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "box = plt.boxplot(measures_df[['mape_esn_fan', 'mape_esn', 'mape_lightgbm', 'mape_prophet',\n",
    "                                'mape_mssa']].values, labels=['ESN_FT', 'ESN', 'LightGBM', 'Prophet', 'SSA'],\n",
    "         patch_artist=True, showfliers=False, meanline=True)\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Закрашиваем каждый ящик своим цветом\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "for median in box['medians']:\n",
    "    median.set_color('black')  # Красный цвет медианы\n",
    "    median.set_linewidth(0.5)  # Можно также изменить толщину\n",
    "\n",
    "plt.title(\"Box-Plots of models MAPE\")\n",
    "plt.ylabel('MAPE, %')\n",
    "plt.xlabel('Models')\n",
    "plt.ylim((0,35))\n",
    "plt.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df[['time_spent_esn_f','time_spent_esn','time_spent_lightgbm', 'time_spent_prophet','time_spent_mssa']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters=2, random_state=23)\n",
    "km.fit(measures_df[['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']])\n",
    "measures_df['label']=km.labels_\n",
    "m_df2 = measures_df.copy()\n",
    "m_df2.drop(m_df2[m_df2.mape_esn>100].index, inplace=True)\n",
    "m_df2.drop(m_df2[m_df2.mape_esn_fan>100].index, inplace=True)\n",
    "m_df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(m_df2.groupby('label')[['mape_esn_fan','mape_esn','mape_lightgbm', 'mape_prophet','mape_mssa']].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df2.groupby('label')[['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df2.drop(m_df2[m_df2.hurst < 0.2].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df2['dif']=m_df2['mape_esn_fan']-m_df2['mape_lightgbm']\n",
    "d0=m_df2[(m_df2.dif<-1)&((m_df2.label==0))]\n",
    "d1=m_df2[(m_df2.dif>1)&(m_df2.label==0)]\n",
    "n=len(m_df2[m_df2.label==0])\n",
    "print('Резервуар лучше: %.1f%%; хуже %.1f%%'%(len(d0)/n*100, len(d1)/n*100), n)\n",
    "t=['Hurst exp.', 'Noise factor', 'Correlation dim.', 'Lyapunov exp.', 'KS-entropy', 'Fourier harmonics']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i, m in enumerate(['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.boxplot([d0[m], d1[m]], positions=[1,2], widths=.7, showfliers=False)\n",
    "    plt.xticks([1,2],['ESN-FT','LightGBM'])\n",
    "    plt.ylabel(t[i])\n",
    "    plt.grid(axis='y')\n",
    "    print(m, '\\t%.2f %.2f'%(d0[m].median(), d1[m].median()))\n",
    "plt.tight_layout()\n",
    "# plt.savefig('competition.jpg',transparent=None, dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = ['Hurst exp.', 'Noise factor', 'Correlation dim.', 'Lyapunov exp.', 'KS-entropy', 'Fourier harmonics']\n",
    "colors = ['#1f77b4', '#2ca02c']  # Цвета для ESN-FT и LightGBM\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, m in enumerate(['hurst', 'noise_factor', 'corr_dim', 'lyap_max', 'ks_entropy', 'fourier_harmonics']):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    box = plt.boxplot([d0[m], d1[m]], positions=[1, 2], widths=.7,\n",
    "                      patch_artist=True, showfliers=False)\n",
    "\n",
    "    # Окрашиваем каждый ящик в свой цвет\n",
    "    for patch, color in zip(box['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # Настройка медианы\n",
    "    for median in box['medians']:\n",
    "        median.set_color('black')\n",
    "        median.set_linewidth(0.8)\n",
    "\n",
    "    plt.xticks([1, 2], ['ESN-FT', 'LightGBM'])\n",
    "    plt.ylabel(t[i])\n",
    "    plt.grid(axis='y')\n",
    "    print(m, '\\t%.2f %.2f' % (d0[m].median(), d1[m].median()))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
